<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Simple Google Speech Test</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.6;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
      margin-bottom: 20px;
    }
    .card {
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 20px;
      margin-bottom: 20px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    .form-group {
      margin-bottom: 15px;
    }
    label {
      display: block;
      margin-bottom: 5px;
      font-weight: 500;
    }
    input {
      width: 100%;
      padding: 8px;
      border: 1px solid #ccc;
      border-radius: 4px;
      font-size: 16px;
    }
    select {
      width: 100%;
      padding: 8px;
      border: 1px solid #ccc;
      border-radius: 4px;
      font-size: 16px;
    }
    button {
      background-color: #1a73e8;
      color: white;
      border: none;
      border-radius: 4px;
      padding: 10px 20px;
      font-size: 16px;
      cursor: pointer;
      transition: background-color 0.3s;
    }
    button:hover {
      background-color: #0d47a1;
    }
    button:disabled {
      background-color: #cccccc;
      cursor: not-allowed;
    }
    .status {
      font-weight: bold;
      margin: 10px 0;
    }
    .error {
      color: #d32f2f;
    }
    .success {
      color: #388e3c;
    }
    .transcript {
      border: 1px solid #e0e0e0;
      border-radius: 4px;
      padding: 15px;
      min-height: 100px;
      background-color: #f9f9f9;
      margin-top: 10px;
    }
    .audio-level {
      height: 10px;
      background-color: #e0e0e0;
      border-radius: 5px;
      margin: 10px 0;
      overflow: hidden;
    }
    .audio-level-fill {
      height: 100%;
      width: 0%;
      background-color: #1a73e8;
      transition: width 0.1s;
    }
  </style>
</head>
<body>
  <h1>Simple Google Speech Test</h1>
  
  <div class="card">
    <div class="form-group">
      <label for="api-key">Google Cloud API Key:</label>
      <input type="password" id="api-key" placeholder="Enter your Google Cloud API Key">
    </div>
    
    <div class="form-group">
      <label for="language">Language:</label>
      <select id="language">
        <option value="en-US">English (US)</option>
        <option value="en-GB">English (UK)</option>
        <option value="zh-CN">Chinese (Simplified)</option>
        <option value="zh-TW">Chinese (Traditional)</option>
        <option value="fr-FR">French</option>
        <option value="de-DE">German</option>
        <option value="ja-JP">Japanese</option>
        <option value="ko-KR">Korean</option>
        <option value="es-ES">Spanish</option>
        <option value="ru-RU">Russian</option>
      </select>
    </div>
    
    <div class="form-group">
      <label for="model">Speech Recognition Model:</label>
      <select id="model">
        <option value="default">Default</option>
        <option value="command_and_search">Command and Search</option>
        <option value="phone_call">Phone Call</option>
        <option value="video">Video</option>
        <option value="latest_short">Latest Short</option>
        <option value="latest_long">Latest Long</option>
      </select>
    </div>
    
    <div class="status" id="status">Ready to test</div>
    <div class="audio-level">
      <div class="audio-level-fill" id="audio-level-fill"></div>
    </div>
    
    <button id="record-button">Start Recording</button>
    <button id="clear-button">Clear Results</button>
  </div>
  
  <div class="card">
    <h2>Transcript</h2>
    <div class="transcript" id="transcript">No results yet</div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // DOM Elements
      const apiKeyInput = document.getElementById('api-key');
      const languageSelect = document.getElementById('language');
      const modelSelect = document.getElementById('model');
      const statusDiv = document.getElementById('status');
      const recordButton = document.getElementById('record-button');
      const clearButton = document.getElementById('clear-button');
      const transcriptDiv = document.getElementById('transcript');
      const audioLevelFill = document.getElementById('audio-level-fill');
      
      // Variables
      let mediaRecorder;
      let audioChunks = [];
      let isRecording = false;
      let audioContext;
      let analyser;
      let audioData;
      let visualizationRequestId;
      let apiKey = '';
      let microphoneStream = null; // Store the stream to reuse it
      
      // Initialize
      async function init() {
        // Load API key from localStorage if available
        const savedApiKey = localStorage.getItem('googleSpeechApiKey');
        if (savedApiKey) {
          apiKeyInput.value = savedApiKey;
          apiKey = savedApiKey;
          statusDiv.textContent = 'API key loaded from storage';
        }
        
        // Save API key when entered
        apiKeyInput.addEventListener('change', function() {
          apiKey = apiKeyInput.value.trim();
          if (apiKey) {
            localStorage.setItem('googleSpeechApiKey', apiKey);
            statusDiv.textContent = 'API key saved';
          }
        });
        
        // Request microphone permission once at startup
        try {
          statusDiv.textContent = 'Requesting microphone access...';
          microphoneStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          statusDiv.textContent = 'Microphone access granted';
          
          // Initialize audio visualization
          initAudioVisualization(microphoneStream);
        } catch (error) {
          statusDiv.textContent = `Microphone error: ${error.message}`;
          statusDiv.className = 'status error';
          console.error('Microphone access error:', error);
        }
      }
      
      // Record button click handler
      recordButton.addEventListener('click', async function() {
        if (isRecording) {
          // Stop recording
          stopRecording();
        } else {
          // Start recording
          await startRecording();
        }
      });
      
      // Clear button click handler
      clearButton.addEventListener('click', function() {
        transcriptDiv.textContent = 'No results yet';
        statusDiv.textContent = 'Results cleared';
      });
      
      // Start recording
      function startRecording() {
        // Check if API key is provided
        if (!apiKeyInput.value.trim()) {
          statusDiv.textContent = 'Please enter your Google Cloud API Key';
          statusDiv.className = 'status error';
          return;
        }
        
        // Check if we have microphone access
        if (!microphoneStream) {
          statusDiv.textContent = 'No microphone access. Please reload the page.';
          statusDiv.className = 'status error';
          return;
        }
        
        try {
          // Reset
          audioChunks = [];
          
          // Create media recorder using the existing stream
          mediaRecorder = new MediaRecorder(microphoneStream);
          
          // Set up event handlers
          mediaRecorder.ondataavailable = function(event) {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };
          
          mediaRecorder.onstop = async function() {
            // Process the recorded audio
            statusDiv.textContent = 'Processing audio...';
            await processAudio();
            
            // Update UI
            isRecording = false;
            recordButton.textContent = 'Start Recording';
          };
          
          // Start recording
          mediaRecorder.start(100); // Collect data every 100ms
          
          // Update UI
          isRecording = true;
          recordButton.textContent = 'Stop Recording';
          statusDiv.textContent = 'Recording...';
          statusDiv.className = 'status';
          
          // Restart audio visualization if needed
          if (!visualizationRequestId) {
            visualizeAudio();
          }
          
        } catch (error) {
          statusDiv.textContent = `Error: ${error.message}`;
          statusDiv.className = 'status error';
          console.error('Recording error:', error);
        }
      }
      
      // Stop recording
      function stopRecording() {
        if (mediaRecorder && mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
          statusDiv.textContent = 'Stopping recording...';
        }
      }
      
      // Initialize audio visualization
      function initAudioVisualization(stream) {
        try {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          const source = audioContext.createMediaStreamSource(stream);
          analyser = audioContext.createAnalyser();
          analyser.fftSize = 256;
          source.connect(analyser);
          audioData = new Uint8Array(analyser.frequencyBinCount);
          
          // Start visualization
          visualizeAudio();
        } catch (error) {
          console.warn('Audio visualization not supported:', error);
        }
      }
      
      // Visualize audio levels
      function visualizeAudio() {
        if (!analyser) return;
        
        analyser.getByteFrequencyData(audioData);
        let sum = 0;
        for (let i = 0; i < audioData.length; i++) {
          sum += audioData[i];
        }
        const average = sum / audioData.length;
        const level = Math.min(100, average * 100 / 256);
        
        if (audioLevelFill) {
          audioLevelFill.style.width = `${level}%`;
        }
        
        visualizationRequestId = requestAnimationFrame(visualizeAudio);
      }
      
      // Process recorded audio
      async function processAudio() {
        if (audioChunks.length === 0) {
          statusDiv.textContent = 'Error: No audio recorded';
          statusDiv.className = 'status error';
          return;
        }
        
        try {
          // Create blob from audio chunks
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          
          // Convert blob to base64
          const reader = new FileReader();
          reader.readAsDataURL(audioBlob);
          
          reader.onloadend = async function() {
            // Get base64 data (remove the data URL prefix)
            const base64Audio = reader.result.split(',')[1];
            
            // Create request to Google Cloud Speech API
            const request = {
              config: {
                languageCode: languageSelect.value,
                model: modelSelect.value,
                enableAutomaticPunctuation: true
              },
              audio: {
                content: base64Audio
              }
            };
            
            // Send request to Google Cloud Speech API
            try {
              statusDiv.textContent = 'Sending to Google...';
              
              const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=${apiKey}`, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json'
                },
                body: JSON.stringify(request)
              });
              
              const data = await response.json();
              
              if (data.error) {
                throw new Error(data.error.message || 'Unknown API error');
              }
              
              // Process results
              if (data.results && data.results.length > 0) {
                // Display result
                const transcript = data.results.map(result => 
                  result.alternatives[0].transcript
                ).join(' ');
                
                // Display confidence if available
                let confidenceInfo = '';
                if (data.results[0].alternatives[0].confidence) {
                  const confidence = (data.results[0].alternatives[0].confidence * 100).toFixed(2);
                  confidenceInfo = ` (Confidence: ${confidence}%)`;
                }
                
                transcriptDiv.textContent = transcript;
                statusDiv.textContent = `Recognition complete with model: ${modelSelect.value}${confidenceInfo}`;
                statusDiv.className = 'status success';
              } else {
                transcriptDiv.textContent = 'No speech detected';
                statusDiv.textContent = 'No speech detected';
              }
              
            } catch (error) {
              transcriptDiv.textContent = 'Error processing speech';
              statusDiv.textContent = `Error: ${error.message}`;
              statusDiv.className = 'status error';
              console.error('API error:', error);
            }
          };
          
        } catch (error) {
          statusDiv.textContent = `Error processing audio: ${error.message}`;
          statusDiv.className = 'status error';
          console.error('Processing error:', error);
        }
      }
      
      // Initialize the page
      init();
    });
  </script>
</body>
</html>
